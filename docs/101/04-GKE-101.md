# GKE 101: Google Kubernetes Engine

## Table of Contents
1. [What is GKE?](#what-is-gke)
2. [GKE vs Local Kubernetes](#gke-vs-local-kubernetes)
3. [Core Concepts](#core-concepts)
4. [Creating Your First Cluster](#creating-your-first-cluster)
5. [Node Pools](#node-pools)
6. [Networking](#networking)
7. [Security](#security)
8. [Hands-on Exercises](#hands-on-exercises)
9. [Cost Management](#cost-management)

---

## What is GKE?

**Google Kubernetes Engine (GKE)** is Google's managed Kubernetes service. It provides:
- **Managed Control Plane**: Google manages the Kubernetes master nodes
- **Auto-scaling**: Automatically scale nodes based on demand
- **Auto-upgrades**: Automatic Kubernetes version updates
- **Cloud Integration**: Native integration with GCP services
- **High Availability**: Multi-zone and regional clusters

### Why GKE?

**Local Kubernetes:**
- ✅ Great for learning
- ✅ Free
- ✅ Fast iteration
- ❌ Limited resources
- ❌ No high availability
- ❌ Manual management

**GKE:**
- ✅ Production-ready
- ✅ Auto-scaling
- ✅ Managed control plane
- ✅ Cloud integrations
- ✅ High availability
- ✅ Load balancers
- ❌ Costs money (~$70/month + nodes)

---

## GKE vs Local Kubernetes

### Architecture Comparison

**Local (minikube/kind):**
```
Your Machine
└── Single Node
    ├── Control Plane
    └── Worker Node
```

**GKE:**
```
Google Cloud
├── Managed Control Plane (Google manages)
└── Your Node Pool
    ├── Node 1 (VM)
    ├── Node 2 (VM)
    └── Node 3 (VM)
```

### Feature Comparison

| Feature | Local | GKE |
|---------|-------|-----|
| Control Plane | You manage | Google manages |
| Nodes | Your machine | GCP VMs |
| Auto-scaling | Manual | Automatic |
| Load Balancer | NodePort | Cloud Load Balancer |
| Storage | Local | Persistent Disks |
| Networking | Basic | VPC, Cloud NAT |
| Cost | Free | ~$70/month + nodes |

---

## Core Concepts

### 1. Cluster
A **GKE cluster** consists of:
- **Control Plane**: Managed by Google (you don't see it)
- **Node Pool**: Your worker nodes (VMs you pay for)

### 2. Node Pool
A **node pool** is a group of nodes with the same configuration.

```
Cluster
└── Node Pool: default-pool
    ├── Node 1 (e2-medium, us-central1-a)
    ├── Node 2 (e2-medium, us-central1-b)
    └── Node 3 (e2-medium, us-central1-c)
```

**You can have multiple node pools:**
```
Cluster
├── Node Pool: default-pool (general workloads)
├── Node Pool: gpu-pool (GPU workloads)
└── Node Pool: spot-pool (preemptible VMs)
```

### 3. Autoscaling
**Cluster Autoscaler** automatically adds/removes nodes based on demand.

```yaml
autoscaling:
  minNodeCount: 1
  maxNodeCount: 10
```

### 4. Workload Identity
**Workload Identity** allows pods to access GCP services securely without service account keys.

```
Kubernetes Service Account
    ↓ (Workload Identity Binding)
GCP Service Account
    ↓
GCP Services (Cloud SQL, Secret Manager, etc.)
```

---

## Creating Your First Cluster

### Prerequisites

1. **Google Cloud Account**
   ```bash
   gcloud auth login
   gcloud config set project YOUR_PROJECT_ID
   ```

2. **Enable APIs**
   ```bash
   gcloud services enable container.googleapis.com
   gcloud services enable compute.googleapis.com
   ```

3. **Set Default Region**
   ```bash
   gcloud config set compute/region us-central1
   gcloud config set compute/zone us-central1-a
   ```

### Method 1: Using gcloud CLI

```bash
gcloud container clusters create my-first-cluster \
  --num-nodes=3 \
  --machine-type=e2-medium \
  --region=us-central1 \
  --enable-autoscaling \
  --min-nodes=1 \
  --max-nodes=5
```

**What this creates:**
- 3-node cluster in `us-central1`
- Nodes are `e2-medium` VMs
- Autoscaling enabled (1-5 nodes)
- Default node pool

### Method 2: Using Terraform (Recommended)

Create `gke-cluster.tf`:

```hcl
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

provider "google" {
  project = var.project_id
  region  = "us-central1"
}

resource "google_container_cluster" "primary" {
  name     = "my-first-cluster"
  location = "us-central1"

  # Remove default node pool (we'll create our own)
  remove_default_node_pool = true
  initial_node_count       = 1

  # Enable autoscaling
  node_pool {
    name       = "default-pool"
    node_count = 3

    autoscaling {
      min_node_count = 1
      max_node_count = 5
    }

    node_config {
      machine_type = "e2-medium"
      disk_size_gb = 50
    }
  }
}
```

Apply:
```bash
terraform init
terraform plan
terraform apply
```

### Method 3: Using GCP Console

1. Go to **Kubernetes Engine → Clusters**
2. Click **Create Cluster**
3. Choose **Standard** cluster
4. Configure:
   - Name: `my-first-cluster`
   - Location: `us-central1`
   - Nodes: 3
   - Machine type: `e2-medium`
5. Click **Create**

### Connect to Your Cluster

```bash
# Get credentials
gcloud container clusters get-credentials my-first-cluster \
  --region=us-central1

# Verify connection
kubectl get nodes
kubectl cluster-info
```

---

## Node Pools

### Understanding Node Pools

A **node pool** is a group of nodes with the same configuration.

**Why multiple node pools?**
- Different machine types for different workloads
- GPU nodes for ML workloads
- Spot/preemptible nodes for cost savings
- Different zones for high availability

### Creating a Node Pool

**Using gcloud:**
```bash
gcloud container node-pools create gpu-pool \
  --cluster=my-first-cluster \
  --region=us-central1 \
  --machine-type=n1-standard-4 \
  --num-nodes=2 \
  --accelerator=type=nvidia-tesla-k80,count=1 \
  --enable-autoscaling \
  --min-nodes=0 \
  --max-nodes=5
```

**Using Terraform:**
```hcl
resource "google_container_node_pool" "gpu_pool" {
  name       = "gpu-pool"
  location   = "us-central1"
  cluster    = google_container_cluster.primary.name
  node_count = 2

  autoscaling {
    min_node_count = 0
    max_node_count = 5
  }

  node_config {
    machine_type = "n1-standard-4"
    
    guest_accelerator {
      type  = "nvidia-tesla-k80"
      count = 1
    }
  }
}
```

### Managing Node Pools

```bash
# List node pools
gcloud container node-pools list \
  --cluster=my-first-cluster \
  --region=us-central1

# Describe node pool
gcloud container node-pools describe default-pool \
  --cluster=my-first-cluster \
  --region=us-central1

# Resize node pool
gcloud container clusters resize my-first-cluster \
  --node-pool=default-pool \
  --num-nodes=5 \
  --region=us-central1

# Delete node pool
gcloud container node-pools delete gpu-pool \
  --cluster=my-first-cluster \
  --region=us-central1
```

### Autoscaling

**Cluster Autoscaler** automatically adjusts node count:

```yaml
autoscaling:
  minNodeCount: 1    # Minimum nodes (always running)
  maxNodeCount: 10   # Maximum nodes (scale up limit)
```

**How it works:**
- Pods can't be scheduled → Add nodes
- Nodes underutilized → Remove nodes
- Respects min/max limits

**Enable autoscaling:**
```bash
gcloud container clusters update my-first-cluster \
  --enable-autoscaling \
  --min-nodes=1 \
  --max-nodes=10 \
  --node-pool=default-pool \
  --region=us-central1
```

---

## Networking

### VPC-Native Clusters

GKE clusters can be **VPC-native** (recommended):
- Pods get IPs from VPC subnet
- Direct access to VPC resources
- Better performance

### Private Clusters

**Private clusters** have:
- Private nodes (no external IPs)
- Private endpoint (control plane not publicly accessible)
- More secure

```hcl
resource "google_container_cluster" "private" {
  name     = "private-cluster"
  location = "us-central1"

  private_cluster_config {
    enable_private_nodes    = true
    enable_private_endpoint = true
    master_ipv4_cidr_block  = "172.16.0.0/28"
  }
}
```

### Load Balancing

**GKE provides:**
- **LoadBalancer Service**: Creates Google Cloud Load Balancer
- **Ingress**: HTTP(S) load balancing with SSL termination

**Example LoadBalancer:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: my-app
```

**Example Ingress:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-app-ingress
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-app-service
            port:
              number: 80
```

---

## Security

### Workload Identity

**Workload Identity** allows pods to access GCP services without keys.

**Setup:**

1. **Enable Workload Identity on cluster:**
```hcl
resource "google_container_cluster" "primary" {
  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }
}
```

2. **Create GCP Service Account:**
```bash
gcloud iam service-accounts create my-app-sa \
  --display-name="My App Service Account"

gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:my-app-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

3. **Create Kubernetes Service Account:**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app-sa
  namespace: default
  annotations:
    iam.gke.io/gcp-service-account: my-app-sa@PROJECT_ID.iam.gserviceaccount.com
```

4. **Bind them:**
```bash
gcloud iam service-accounts add-iam-policy-binding \
  my-app-sa@PROJECT_ID.iam.gserviceaccount.com \
  --role roles/iam.workloadIdentityUser \
  --member "serviceAccount:PROJECT_ID.svc.id.goog[default/my-app-sa]"
```

5. **Use in Pod:**
```yaml
apiVersion: v1
kind: Pod
spec:
  serviceAccountName: my-app-sa
  containers:
  - name: my-app
    image: my-app:latest
```

### Network Policies

**Network Policies** control pod-to-pod communication:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

Enable on cluster:
```hcl
resource "google_container_cluster" "primary" {
  network_policy {
    enabled  = true
    provider = "CALICO"
  }
}
```

---

## Hands-on Exercises

### Exercise 1: Create and Deploy to GKE

```bash
# Create cluster
gcloud container clusters create my-cluster \
  --num-nodes=2 \
  --machine-type=e2-medium \
  --region=us-central1

# Get credentials
gcloud container clusters get-credentials my-cluster --region=us-central1

# Deploy app
kubectl create deployment hello-world --image=gcr.io/google-samples/hello-app:1.0

# Expose service
kubectl expose deployment hello-world \
  --type=LoadBalancer \
  --port=80 \
  --target-port=8080

# Get external IP
kubectl get service hello-world
```

### Exercise 2: Enable Autoscaling

```bash
# Enable autoscaling
gcloud container clusters update my-cluster \
  --enable-autoscaling \
  --min-nodes=1 \
  --max-nodes=5 \
  --node-pool=default-pool \
  --region=us-central1

# Scale deployment to trigger autoscaling
kubectl scale deployment hello-world --replicas=10

# Watch nodes scale up
kubectl get nodes -w
```

### Exercise 3: Create Multiple Node Pools

```bash
# Create GPU node pool
gcloud container node-pools create gpu-pool \
  --cluster=my-cluster \
  --region=us-central1 \
  --machine-type=n1-standard-4 \
  --num-nodes=1 \
  --enable-autoscaling \
  --min-nodes=0 \
  --max-nodes=3

# Schedule pod on GPU node
kubectl run gpu-pod \
  --image=nvidia/cuda:11.0-base \
  --overrides='{"apiVersion":"v1","spec":{"nodeSelector":{"cloud.google.com/gke-nodepool":"gpu-pool"}}}'
```

### Exercise 4: Use Workload Identity

```bash
# Enable Workload Identity (if not already)
gcloud container clusters update my-cluster \
  --workload-pool=PROJECT_ID.svc.id.goog \
  --region=us-central1

# Create GCP service account
gcloud iam service-accounts create my-app-sa

# Grant permissions
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:my-app-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"

# Create K8s service account with annotation
kubectl create serviceaccount my-app-sa \
  --dry-run=client -o yaml | \
  kubectl annotate --local -f - \
    iam.gke.io/gcp-service-account=my-app-sa@PROJECT_ID.iam.gserviceaccount.com -o yaml | \
  kubectl apply -f -

# Bind them
gcloud iam service-accounts add-iam-policy-binding \
  my-app-sa@PROJECT_ID.iam.gserviceaccount.com \
  --role roles/iam.workloadIdentityUser \
  --member "serviceAccount:PROJECT_ID.svc.id.goog[default/my-app-sa]"
```

---

## Cost Management

### Understanding Costs

**GKE Costs:**
- **Control Plane**: ~$70/month (always running)
- **Nodes**: Pay per VM instance
  - e2-medium: ~$30/month
  - e2-standard-2: ~$60/month
- **Storage**: Persistent disks (~$0.17/GB/month)
- **Load Balancer**: ~$18/month + traffic

### Cost Optimization

1. **Use Autoscaling**
   ```bash
   # Scale down to 0 when not in use
   gcloud container node-pools update default-pool \
     --cluster=my-cluster \
     --enable-autoscaling \
     --min-nodes=0 \
     --max-nodes=5
   ```

2. **Use Preemptible/Spot VMs**
   ```hcl
   node_config {
     preemptible = true  # 80% cheaper, can be terminated
   }
   ```

3. **Right-size Nodes**
   - Use smaller machine types when possible
   - Monitor resource usage
   - Adjust based on actual needs

4. **Delete Unused Clusters**
   ```bash
   # Stop billing completely
   gcloud container clusters delete my-cluster --region=us-central1
   ```

5. **Use Regional Persistent Disks**
   - More expensive but better for HA
   - Use zonal disks for dev/test

### Monitoring Costs

```bash
# View cluster costs in GCP Console
# Billing → Reports → Filter by "Kubernetes Engine"
```

---

## Project-Specific: Our GKE Setup

Looking at this project's GKE configuration:

**Key Features:**
- **VPC-native cluster**: Pods get VPC IPs
- **Private nodes**: More secure
- **Workload Identity**: Secure GCP access
- **Autoscaling**: 1-5 nodes
- **Multiple environments**: Dev, staging, prod

**Try It:**
```bash
cd infrastructure/terraform/environments/dev

# Review configuration
cat main.tf

# Plan cluster creation
terraform plan

# Create cluster
terraform apply
```

---

## Common Commands

```bash
# List clusters
gcloud container clusters list

# Get credentials
gcloud container clusters get-credentials CLUSTER_NAME --region=REGION

# Describe cluster
gcloud container clusters describe CLUSTER_NAME --region=REGION

# Update cluster
gcloud container clusters update CLUSTER_NAME --region=REGION

# Delete cluster
gcloud container clusters delete CLUSTER_NAME --region=REGION

# Node pools
gcloud container node-pools list --cluster=CLUSTER_NAME --region=REGION
gcloud container node-pools describe POOL_NAME --cluster=CLUSTER_NAME --region=REGION
```

---

## Next Steps

1. ✅ Create your first GKE cluster
2. ✅ Deploy applications
3. ✅ Set up autoscaling
4. ✅ Configure Workload Identity
5. ✅ Learn [Monitoring](./06-PROMETHEUS-GRAFANA-101.md)

---

## Additional Resources

- [GKE Official Docs](https://cloud.google.com/kubernetes-engine/docs)
- [GKE Best Practices](https://cloud.google.com/kubernetes-engine/docs/best-practices)
- [GKE Pricing](https://cloud.google.com/kubernetes-engine/pricing)
- [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity)

---

**Congratulations!** You now understand GKE. Next, learn [Monitoring](./06-PROMETHEUS-GRAFANA-101.md) to observe your applications!

